# üöÄ GPU Serverless Deployment Guide

**–ë—ã—Å—Ç—Ä–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Chatterbox TTS –Ω–∞ RunPod GPU Serverless**

## üìã –ß–µ–∫-–ª–∏—Å—Ç —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

### ‚úÖ 1. GitHub –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ (2 –º–∏–Ω—É—Ç—ã)
```bash
# –§–æ—Ä–∫–Ω–∏—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ GitHub
# GitHub Actions –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–µ—Ä–µ—Ç Docker –æ–±—Ä–∞–∑
```

### ‚úÖ 2. RunPod Serverless (5 –º–∏–Ω—É—Ç)

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:**
```
Container Image: ghcr.io/your-username/chatterbox-runpod:latest
GPU Type: RTX 4090 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è) –∏–ª–∏ A100
Memory: 16GB+
Container Disk: 25GB+
```

**–≠–∫–æ–Ω–æ–º–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:**
```
Min Workers: 0
Max Workers: 3
Idle Timeout: 5 —Å–µ–∫—É–Ω–¥
Max Execution Time: 300 —Å–µ–∫—É–Ω–¥
```

### ‚úÖ 3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (1 –º–∏–Ω—É—Ç–∞)
```python
import runpod
runpod.api_key = "YOUR_API_KEY"
endpoint = runpod.Endpoint("YOUR_ENDPOINT_ID")

# –¢–µ—Å—Ç
response = endpoint.run({
    "text": "–ü—Ä–∏–≤–µ—Ç! GPU TTS —Ä–∞–±–æ—Ç–∞–µ—Ç!",
    "model_type": "multilingual",
    "language_id": "ru"
})
print(f"‚úÖ –£—Å–ø–µ—Ö! GPU –≤—Ä–µ–º—è: {response['gpu_time']:.2f}s")
```

## üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ GPU –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### üí° –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
```
GPU: RTX 3090
Memory: 16GB
Cost: ~$0.34/—á–∞—Å
Throughput: 6-8 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω
```

### ‚≠ê –î–ª—è production
```
GPU: RTX 4090  
Memory: 24GB
Cost: ~$0.79/—á–∞—Å
Throughput: 10-15 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω
```

### üöÄ –î–ª—è –≤—ã—Å–æ–∫–∏—Ö –Ω–∞–≥—Ä—É–∑–æ–∫
```
GPU: A100 40GB
Memory: 40GB
Cost: ~$1.89/—á–∞—Å
Throughput: 15-20 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
- **Cold Start**: 30-60 —Å–µ–∫—É–Ω–¥ (–ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å)
- **GPU Time**: 1-5 —Å–µ–∫—É–Ω–¥ –Ω–∞ –∑–∞–ø—Ä–æ—Å
- **GPU Memory**: 6-12GB –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
- **Throughput**: –ó–∞–≤–∏—Å–∏—Ç –æ—Ç GPU

### –õ–æ–≥–∏ RunPod:
```
Console ‚Üí Serverless ‚Üí Endpoint ‚Üí Logs

–ò—â–∏—Ç–µ:
‚úÖ "All models loaded successfully on GPU"
üéØ "TTS Request: ..."
‚úÖ "Generated 2.5s audio in 1.2s GPU time"
```

## üí∞ –û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏

### –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞—Å—á–µ—Ç:
```
1000 –∑–∞–ø—Ä–æ—Å–æ–≤ √ó 3 —Å–µ–∫ GPU –≤—Ä–µ–º—è = 50 –º–∏–Ω—É—Ç GPU
RTX 4090: 50 –º–∏–Ω √ó $0.79/—á–∞—Å = ~$0.66
A100: 50 –º–∏–Ω √ó $1.89/—á–∞—Å = ~$1.58
```

### –≠–∫–æ–Ω–æ–º–∏—è:
- ‚úÖ Idle Timeout 5 —Å–µ–∫ = –∞–≤—Ç–æ–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ
- ‚úÖ Min Workers 0 = –ø–ª–∞—Ç–∏—Ç–µ —Ç–æ–ª—å–∫–æ –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ  
- ‚úÖ Batch –∑–∞–ø—Ä–æ—Å—ã = –º–µ–Ω—å—à–µ cold starts

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

### –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è:
```python
{
    "exaggeration": 0.3,
    "cfg_weight": 0.7
}
```

### –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ:
```python
{
    "exaggeration": 0.5,
    "cfg_weight": 0.5  
}
```

### –î—Ä–∞–º–∞—Ç–∏—á–Ω–∞—è —Ä–µ—á—å:
```python
{
    "exaggeration": 0.8,
    "cfg_weight": 0.3
}
```

## üêõ –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### "CUDA not available"
- ‚úÖ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—ã–±–æ—Ä GPU –≤ RunPod
- ‚úÖ –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ GPU endpoint

### "GPU out of memory"  
- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ –º–æ—â–Ω—ã–π GPU
- ‚úÖ –°–æ–∫—Ä–∞—Ç–∏—Ç–µ –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ (<5000 —Å–∏–º–≤–æ–ª–æ–≤)
- ‚úÖ –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ container

### "Container failed to start"
- ‚úÖ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Docker –æ–±—Ä–∞–∑ –≤ GitHub Registry
- ‚úÖ –£–≤–µ–ª–∏—á—å—Ç–µ Container Disk –¥–æ 30GB+

### –ú–µ–¥–ª–µ–Ω–Ω—ã–π cold start
- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ warm workers (Min Workers > 0)
- ‚úÖ –ì—Ä—É–ø–ø–∏—Ä—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å—ã –≤ batch

## üìû –ë—ã—Å—Ç—Ä–∞—è –ø–æ–º–æ—â—å

- **RunPod Discord**: https://discord.gg/runpod
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: https://docs.runpod.io/serverless
- **Status**: https://status.runpod.io

---

**üéâ –ì–æ—Ç–æ–≤–æ! –í–∞—à GPU TTS API —Ä–∞–±–æ—Ç–∞–µ—Ç!**
