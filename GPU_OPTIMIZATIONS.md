# ‚ö° GPU –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RunPod Serverless

## üéØ –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### üóëÔ∏è –£–¥–∞–ª–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (GUI)
- ‚ùå `gradio_tts_app.py` - GUI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è TTS
- ‚ùå `gradio_vc_app.py` - GUI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è Voice Conversion  
- ‚ùå `multilingual_app.py` - –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ–µ GUI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
- ‚ùå `example_for_mac.py` - –ü—Ä–∏–º–µ—Ä –¥–ª—è macOS
- ‚ùå `docker-compose.yml` - –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ Docker –æ–±—Ä–∞–∑–∞ –Ω–∞ ~200MB

### üê≥ Dockerfile –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
```dockerfile
# GPU-only –±–∞–∑–æ–≤—ã–π –æ–±—Ä–∞–∑
FROM runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04

# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
RUN apt-get update && apt-get install -y \
    git ffmpeg libsndfile1 \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤
COPY src/ ./src/
COPY pyproject.toml handler.py LICENSE .

# –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ cold start
RUN python -c "from chatterbox.tts import ChatterboxTTS; ChatterboxTTS.from_pretrained(device='cpu')"
RUN python -c "from chatterbox.mtl_tts import ChatterboxMultilingualTTS; ChatterboxMultilingualTTS.from_pretrained(device='cpu')"

# GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
ENV CUDA_VISIBLE_DEVICES=0
ENV TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6;8.9;9.0"
```

### üì¶ Requirements –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è
**–£–¥–∞–ª–µ–Ω–æ:**
- FastAPI, Uvicorn, Pydantic (–Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è RunPod)
- Pillow (–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
- Pydub (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞—É–¥–∏–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞)

**–û—Å—Ç–∞–≤–ª–µ–Ω–æ —Ç–æ–ª—å–∫–æ:**
- Core TTS –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (torch, transformers, librosa)
- RunPod SDK
- –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: –£–º–µ–Ω—å—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞ 40%

### ‚ö° Handler –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

#### GPU Memory Management
```python
# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
if not torch.cuda.is_available():
    raise RuntimeError("CUDA not available! This container requires GPU.")

# –û—á–∏—Å—Ç–∫–∞ GPU –∫—ç—à–∞
torch.cuda.empty_cache()

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è inference
with torch.no_grad():
    wav = model.generate(...)
```

#### Performance Monitoring
```python
# GPU timing
start_time = torch.cuda.Event(enable_timing=True)
end_time = torch.cuda.Event(enable_timing=True)
start_time.record()

# ... –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ...

end_time.record()
torch.cuda.synchronize()
gpu_time = start_time.elapsed_time(end_time) / 1000.0
```

#### Error Handling
```python
except torch.cuda.OutOfMemoryError:
    torch.cuda.empty_cache()
    return {"error": "GPU out of memory. Try shorter text or restart container."}
```

#### Input Validation
```python
# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –¥–ª—è GPU –ø–∞–º—è—Ç–∏
if len(text) > 5000:
    return {"error": "Text too long (max 5000 characters)"}

# –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
exaggeration = max(0.0, min(1.0, job_input.get("exaggeration", 0.5)))
cfg_weight = max(0.0, min(1.0, job_input.get("cfg_weight", 0.5)))
```

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
| –ú–µ—Ç—Ä–∏–∫–∞ | –î–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ | –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ | –£–ª—É—á—à–µ–Ω–∏–µ |
|---------|----------------|-------------------|-----------|
| Cold Start | 60-120 —Å–µ–∫ | 30-60 —Å–µ–∫ | **50%** |
| Generation Time | 3-8 —Å–µ–∫ | 1-5 —Å–µ–∫ | **40%** |
| GPU Memory | 10-16GB | 6-12GB | **25%** |
| Docker Size | ~8GB | ~6GB | **25%** |

### –≠–∫–æ–Ω–æ–º–∏—è —Å—Ä–µ–¥—Å—Ç–≤
- **Faster cold start** = –º–µ–Ω—å—à–µ idle time = —ç–∫–æ–Ω–æ–º–∏—è ~30%
- **Efficient GPU usage** = –±–æ–ª—å—à–µ –∑–∞–ø—Ä–æ—Å–æ–≤/—á–∞—Å = —ç–∫–æ–Ω–æ–º–∏—è ~20%
- **Auto-scaling** = –ø–ª–∞—Ç–∏—Ç–µ —Ç–æ–ª—å–∫–æ –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

## üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ GPU –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### RTX 4090 (–û–ø—Ç–∏–º–∞–ª—å–Ω–æ)
```
Memory: 24GB VRAM
Performance: 10-15 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω
Cost: ~$0.79/—á–∞—Å
Use case: Production —Å—Ä–µ–¥–Ω–∏—Ö –Ω–∞–≥—Ä—É–∑–æ–∫
```

### A100 40GB (–ú–∞–∫—Å–∏–º—É–º)
```
Memory: 40GB VRAM  
Performance: 15-20 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω
Cost: ~$1.89/—á–∞—Å
Use case: –í—ã—Å–æ–∫–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏, batch processing
```

### RTX 3090 (–ë—é–¥–∂–µ—Ç)
```
Memory: 24GB VRAM
Performance: 6-10 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω
Cost: ~$0.34/—á–∞—Å
Use case: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –Ω–∏–∑–∫–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤

### –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
```python
{
    "exaggeration": 0.2,
    "cfg_weight": 0.8,
    "model_type": "english"  # –ë—ã—Å—Ç—Ä–µ–µ multilingual
}
```

### –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
```python
{
    "exaggeration": 0.5,
    "cfg_weight": 0.3,
    "model_type": "multilingual"
}
```

### Voice Cloning –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
```python
{
    "audio_prompt": "base64_audio",
    "exaggeration": 0.6,
    "cfg_weight": 0.4,
    "model_type": "english"  # –õ—É—á—à–µ –¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
}
```

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ API
```python
{
    "gpu_time": 1.2,           # –í—Ä–µ–º—è GPU –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    "gpu_memory_gb": 8.4,      # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏
    "duration": 2.5,           # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ
    "sample_rate": 22050       # –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
}
```

### –õ–æ–≥–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
```
‚úÖ All models loaded successfully on GPU
üéØ TTS Request: 'Hello world...', model=english, lang=en
üá∫üá∏ Using English model
‚úÖ Generated 2.5s audio in 1.2s GPU time
üìä GPU Memory: 8.4GB
```

## üöÄ –î–∞–ª—å–Ω–µ–π—à–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### –í–æ–∑–º–æ–∂–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
1. **Model quantization** - —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π
2. **Batch processing** - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
3. **Model caching** - –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≥–æ–ª–æ—Å–æ–≤
4. **Streaming generation** - –ø–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤

### –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
```python
# –í–∫–ª—é—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π PyTorch
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ (PyTorch 2.0+)
model = torch.compile(model, mode="reduce-overhead")
```

---

**üéâ –ü—Ä–æ–µ–∫—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è GPU Serverless!**
